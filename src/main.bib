@incollection{Bengio+chapter2007,
    author = {Bengio, Yoshua and LeCun, Yann},
    booktitle = {Large Scale Kernel Machines},
    publisher = {MIT Press},
    title = {Scaling Learning Algorithms Towards {AI}},
    year = {2007}
}

@article{Hinton06,
    author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
    journal = {Neural Computation},
    pages = {1527--1554},
    title = {A Fast Learning Algorithm for Deep Belief Nets},
    volume = {18},
    year = {2006}
}

@book{goodfellow2016deep,
    title={Deep learning},
    author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
    volume={1},
    year={2016},
    publisher={MIT Press}
}

@article{achiam2023gpt,
    title={Gpt-4 technical report},
    author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
    journal={arXiv preprint arXiv:2303.08774},
    year={2023}
}

@inproceedings{zhai2023stabilizing,
    title={Stabilizing transformer training by preventing attention entropy collapse},
    author={Zhai, Shuangfei and Likhomanenko, Tatiana and Littwin, Etai and Busbridge, Dan and Ramapuram, Jason and Zhang, Yizhe and Gu, Jiatao and Susskind, Joshua M},
    booktitle={International Conference on Machine Learning},
    pages={40770--40803},
    year={2023},
    organization={PMLR}
}

@article{liu2023llm360,
    title={Llm360: Towards fully transparent open-source llms},
    author={Liu, Zhengzhong and Qiao, Aurick and Neiswanger, Willie and Wang, Hongyi and Tan, Bowen and Tao, Tianhua and Li, Junbo and Wang, Yuqi and Sun, Suqi and Pangarkar, Omkar and others},
    journal={arXiv preprint arXiv:2312.06550},
    year={2023}
}

@article{beaini2023towards,
    title={Towards foundational models for molecular learning on large-scale multi-task datasets},
    author={Beaini, Dominique and Huang, Shenyang and Cunha, Joao Alex and Moisescu-Pareja, Gabriela and Dymov, Oleksandr and Maddrell-Mander, Samuel and McLean, Callum and Wenkel, Frederik and M{\"u}ller, Luis and Mohamud, Jama Hussein and others},
    journal={arXiv preprint arXiv:2310.04292},
    year={2023}
}

@article{bordelon2023depthwise,
    title={Depthwise hyperparameter transfer in residual networks: Dynamics and scaling limit},
    author={Bordelon, Blake and Noci, Lorenzo and Li, Mufan Bill and Hanin, Boris and Pehlevan, Cengiz},
    journal={arXiv preprint arXiv:2309.16620},
    year={2023}
}

@article{yaida2022meta,
    title={Meta-principled family of hyperparameter scaling strategies},
    author={Yaida, Sho},
    journal={arXiv preprint arXiv:2210.04909},
    year={2022}
}

@article{hu2024minicpm,
    title={MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies},
    author={Hu, Shengding and Tu, Yuge and Han, Xu and He, Chaoqun and Cui, Ganqu and Long, Xiang and Zheng, Zhi and Fang, Yewei and Huang, Yuxiang and Zhao, Weilin and others},
    journal={arXiv preprint arXiv:2404.06395},
    year={2024}
}

@article{chen2024principled,
    title={Principled Architecture-aware Scaling of Hyperparameters},
    author={Chen, Wuyang and Wu, Junru and Wang, Zhangyang and Hanin, Boris},
    journal={arXiv preprint arXiv:2402.17440},
    year={2024}
}

@article{noci2024learning,
    title={Why do Learning Rates Transfer? Reconciling Optimization and Scaling Limits for Deep Learning},
    author={Noci, Lorenzo and Meterez, Alexandru and Hofmann, Thomas and Orvieto, Antonio},
    journal={arXiv preprint arXiv:2402.17457},
    year={2024}
}

@article{lingle2024large,
    title={A Large-Scale Exploration of $\mu$-Transfer},
    author={Lingle, Lucas},
    journal={arXiv preprint arXiv:2404.05728},
    year={2024}
}

@phdthesis{wortsman2024robust,
    title={Robust and reliable large-scale transfer learning},
    author={Wortsman, Mitchell},
    year={2024}
}

@article{dey2023cerebras,
    title={Cerebras-gpt: Open compute-optimal language models trained on the cerebras wafer-scale cluster},
    author={Dey, Nolan and Gosal, Gurpreet and Khachane, Hemant and Marshall, William and Pathria, Ribhu and Tom, Marvin and Hestness, Joel and others},
    journal={arXiv preprint arXiv:2304.03208},
    year={2023}
}

@article{vyas2024feature,
    title={Feature-learning networks are consistent across widths at realistic scales},
    author={Vyas, Nikhil and Atanasov, Alexander and Bordelon, Blake and Morwani, Depen and Sainathan, Sabarish and Pehlevan, Cengiz},
    journal={Advances in Neural Information Processing Systems},
    volume={36},
    year={2024}
}

@article{yuan2024accelerated,
    title={Accelerated Training via Incrementally Growing Neural Networks using Variance Transfer and Learning Rate Adaptation},
    author={Yuan, Xin and Savarese, Pedro and Maire, Michael},
    journal={Advances in Neural Information Processing Systems},
    volume={36},
    year={2024}
}

@article{li2023flm,
    title={Flm-101b: An open llm and how to train it with \$100 k budget},
    author={Li, Xiang and Yao, Yiqun and Jiang, Xin and Fang, Xuezhi and Meng, Xuying and Fan, Siqi and Han, Peng and Li, Jing and Du, Li and Qin, Bowen and others},
    journal={arXiv preprint arXiv:2309.03852},
    year={2023}
}

@misc{yao2024nanolm,
    title={nanoLM: an Affordable LLM Pre-training Benchmark via Accurate Loss Prediction across Scales}, 
    author={Yiqun Yao and Siqi fan and Xiusheng Huang and Xuezhi Fang and Xiang Li and Ziyi Ni and Xin Jiang and Xuying Meng and Peng Han and Shuo Shang and Kang Liu and Aixin Sun and Yequan Wang},
    year={2024},
    eprint={2304.06875},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@article{chizat2023steering,
    title={Steering deep feature learning with backward aligned feature updates},
    author={Chizat, L{\'e}na{\"\i}c and Netrapalli, Praneeth},
    journal={arXiv preprint arXiv:2311.18718},
    year={2023}
}

@article{dey2023btlm,
    title={Btlm-3b-8k: 7b parameter performance in a 3b parameter model},
    author={Dey, Nolan and Soboleva, Daria and Al-Khateeb, Faisal and Yang, Bowen and Pathria, Ribhu and Khachane, Hemant and Muhammad, Shaheer and Myers, Robert and Steeves, Jacob Robert and Vassilieva, Natalia and others},
    journal={arXiv preprint arXiv:2309.11568},
    year={2023}
}

@article{li2024tele,
    title={Tele-FLM Technical Report},
    author={Li, Xiang and Yao, Yiqun and Jiang, Xin and Fang, Xuezhi and Wang, Chao and Liu, Xinzhang and Wang, Zihan and Zhao, Yu and Wang, Xin and Huang, Yuyao and others},
    journal={arXiv preprint arXiv:2404.16645},
    year={2024}
}

@inproceedings{cabannes2023associative,
    title={Associative Memories with Heavy-Tailed Data},
    author={Cabannes, Vivien and Dohmatob, Elvis and Bietti, Alberto},
    booktitle={NeurIPS 2023 Workshop Heavy Tails in Machine Learning},
    year={2023}
}

@incollection{lecun2002efficient,
    title={Efficient backprop},
    author={LeCun, Yann and Bottou, L{\'e}on and Orr, Genevieve B and M{\"u}ller, Klaus-Robert},
    booktitle={Neural networks: Tricks of the trade},
    pages={9--50},
    year={2002},
    publisher={Springer}
}

% TP0
@article{yang2019scaling,
    title={Scaling limits of wide neural networks with weight sharing: Gaussian process behavior, gradient independence, and neural tangent kernel derivation},
    author={Yang, Greg},
    journal={arXiv preprint arXiv:1902.04760},
    year={2019}
}

% TP1
@article{yang2019wide,
    title={Wide feedforward or recurrent neural networks of any architecture are gaussian processes},
    author={Yang, Greg},
    journal={Advances in Neural Information Processing Systems},
    volume={32},
    year={2019}
}

% TP2
@article{yang2020tensor,
    title={Tensor programs ii: Neural tangent kernel for any architecture},
    author={Yang, Greg},
    journal={arXiv preprint arXiv:2006.14548},
    year={2020}
}

% TP2b
@inproceedings{yang2021tensor,
    title={Tensor programs iib: Architectural universality of neural tangent kernel training dynamics},
    author={Yang, Greg and Littwin, Etai},
    booktitle={International Conference on Machine Learning},
    pages={11762--11772},
    year={2021},
    organization={PMLR}
}

% TP3
@article{yang2020tensor2,
    title={Tensor programs iii: Neural matrix laws},
    author={Yang, Greg},
    journal={arXiv preprint arXiv:2009.10685},
    year={2020}
}

% TP4
@article{yang2020feature,
    title={Feature learning in infinite-width neural networks},
    author={Yang, Greg and Hu, Edward J},
    journal={arXiv preprint arXiv:2011.14522},
    year={2020}
}

% TP4b
@inproceedings{littwin2022adaptive,
    title={Adaptive optimization in the $\infty$-width limit},
    author={Littwin, Etai and Yang, Greg},
    booktitle={The Eleventh International Conference on Learning Representations},
    year={2022}
}

% TP4c
@article{yang2023spectral,
    title={A spectral condition for feature learning},
    author={Yang, Greg and Simon, James B and Bernstein, Jeremy},
    journal={arXiv preprint arXiv:2310.17813},
    year={2023}
}

% TP5
@article{yang2021tuning,
    title={Tuning large neural networks via zero-shot hyperparameter transfer},
    author={Yang, Ge and Hu, Edward and Babuschkin, Igor and Sidor, Szymon and Liu, Xiaodong and Farhi, David and Ryder, Nick and Pachocki, Jakub and Chen, Weizhu and Gao, Jianfeng},
    journal={Advances in Neural Information Processing Systems},
    volume={34},
    pages={17084--17097},
    year={2021}
}

% TP6
@inproceedings{yang2023feature,
    title={Feature Learning in Infinite Depth Neural Networks},
    author={Yang, Greg and Yu, Dingli and Zhu, Chen and Hayou, Soufiane},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2023}
}

@inproceedings{glorot2010understanding,
    title={Understanding the difficulty of training deep feedforward neural networks},
    author={Glorot, Xavier and Bengio, Yoshua},
    booktitle={Proceedings of the thirteenth international conference on artificial intelligence and statistics},
    pages={249--256},
    year={2010},
    organization={JMLR Workshop and Conference Proceedings}
}

@inproceedings{sutskever2013importance,
    title={On the importance of initialization and momentum in deep learning},
    author={Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
    booktitle={International conference on machine learning},
    pages={1139--1147},
    year={2013},
    organization={PMLR}
}
